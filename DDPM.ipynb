{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前言：本代码是根据DDPM论文和网上相关参考资料，从零搭建自己的DDPM模型\n",
    "\n",
    "参考资料：\n",
    "\n",
    "[DDPM原论文](https://arxiv.org/pdf/2006.11239)\n",
    "\n",
    "[图科学实验室](https://mp.weixin.qq.com/s/Rj51LTCjbuX_bn7iALqMIg)\n",
    "\n",
    "[大白话AI](https://www.bilibili.com/video/BV1tz4y1h7q1/?spm_id_from=333.337.search-card.all.click&vd_source=1a02178b1644ddc9b579739c3c1616b4)\n",
    "\n",
    "[手写AI](https://www.bilibili.com/video/BV1BN41117NJ?spm_id_from=333.788.videopod.sections&vd_source=1a02178b1644ddc9b579739c3c1616b4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入相关模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda, Resize\n",
    "from torch.utils.data import DataLoader\n",
    "from natsort import natsorted\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets.mnist import MNIST, FashionMNIST\n",
    "from torchvision.utils import make_grid\n",
    "from einops import rearrange\n",
    "import imageio\n",
    "import yaml\n",
    "import os\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    作用：加载自定义数据集并进行预处理\n",
    "    \"\"\"\n",
    "    def __init__(self, root, transform=None, mode=\"train\"):\n",
    "        \"\"\"\n",
    "        输入：\n",
    "            root:数据集的根目录\n",
    "            transforms:一个列表，包含要应用于图像的转换操作\n",
    "            mode:可以是 \"train\" 或 \"test\"\n",
    "        \"\"\"\n",
    "\n",
    "        # 将传入的 transforms_ 列表组合成一个可以对图像进行转换的函数链\n",
    "        self.transform = transform\n",
    "\n",
    "        # 获取数据集目录下所有图片的路径并进行排序\n",
    "        self.img_paths = natsorted(glob.glob(root + \"/*.png\"))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 通过index取出图像路径并打开图像\n",
    "        img_path = self.img_paths[index]\n",
    "\n",
    "        # 打开图片\n",
    "        img = Image.open(img_path).convert('RGB')  # 以防万一，转成 RGB\n",
    "\n",
    "        # 做transform\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST和FashionMNIST数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    自定义数据集类,用于加载MNIST或FashionMNIST数据集。\n",
    "    \n",
    "    参数:\n",
    "        dataset_name (str): 数据集类型，支持'MNIST'和'FashionMNIST'。\n",
    "        root (str): 数据集存储的根目录。\n",
    "        train (bool): 是否加载训练集。True 加载训练集,False 加载测试集。\n",
    "        transform (callable, optional): 应用于样本的可调用转换。\n",
    "        download (bool): 如果数据集不存在,是否自动下载。\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_name='MNIST', root='./datasets', train=True, transform=None, download=True):\n",
    "        super(MNISTDataset, self).__init__()\n",
    "\n",
    "        self.dataset_name = dataset_name.lower()\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "\n",
    "        if self.dataset_name == 'mnist':\n",
    "            self.dataset = MNIST(\n",
    "                root=root,\n",
    "                train=self.train,\n",
    "                transform=self.transform,\n",
    "                download=download\n",
    "            )\n",
    "        elif self.dataset_name == 'fashionmnist':\n",
    "            self.dataset = FashionMNIST(\n",
    "                root=root,\n",
    "                train=self.train,\n",
    "                transform=self.transform,\n",
    "                download=download\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported dataset_name. Choose either 'MNIST' or 'FashionMNIST'.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        根据索引返回数据(包含数据和索引)\n",
    "        \"\"\"\n",
    "        data = self.dataset[index]\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义前向加噪过程和反向去噪过程\n",
    "\n",
    "前向加噪过程\n",
    "$$\n",
    "x_t = \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\epsilon + \\sqrt{\\bar{\\alpha}_t} \\cdot x_0\n",
    "$$\n",
    "\n",
    "反向去噪过程\n",
    "$$\n",
    "P(X_{t-1} \\mid X_t, X_0) \\sim \\mathcal{N} \\left( \\frac{1}{\\sqrt{\\alpha_t}} \\left( X_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon \\right), \\frac{\\beta_t (1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 噪声调度器\n",
    "class LinearNoiseScheduler:\n",
    "    def __init__(self, num_timesteps, beta_start, beta_end):\n",
    "        \"\"\"\n",
    "        num_timesteps: 模型的时间步数，表示去噪过程中的离散时间步总数\n",
    "        beta_start: 初始的噪声强度（β）值\n",
    "        beta_end: 最终的噪声强度（β）值\n",
    "        \"\"\"\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        \n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alpha_cum_prod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n",
    "        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - self.alpha_cum_prod)\n",
    "\n",
    "    def add_noise(self, original, noise, t):\n",
    "        \"\"\"\n",
    "        original:输入的原始图像,形状为 (batch_size, channels, height, width) 的张量\n",
    "        noise:添加的噪声,形状为 (batch_size, channels, height, width) 的张量\n",
    "        t:当前的时间步\n",
    "        \"\"\"\n",
    "        original_shape = original.shape\n",
    "        batch_size = original_shape[0]\n",
    "\n",
    "        # 获取对应时间步的噪声强度\n",
    "        sqrt_alpha_cum_prod = self.sqrt_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n",
    "        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n",
    "\n",
    "        \n",
    "        # 将噪声强度的维度从(batch_size,)扩展为(batch_size, 1, 1, 1)\n",
    "        for _ in range(len(original_shape) - 1):\n",
    "            sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.unsqueeze(-1)\n",
    "        for _ in range(len(original_shape) - 1):\n",
    "            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n",
    "        \n",
    "        # 根据前向加噪公式进行加噪\n",
    "        return (sqrt_alpha_cum_prod.to(original.device) * original\n",
    "                + sqrt_one_minus_alpha_cum_prod.to(original.device) * noise)\n",
    "    \n",
    "    # 反向去噪过程\n",
    "    def sample_prev_timestep(self, xt, noise_pred, t):\n",
    "        \"\"\"\n",
    "        xt: 当前时间步t的图像,形状为(batch_size, channels, height, width)\n",
    "        noise_pred: 模型预测的噪声,形状与xt相同\n",
    "        t: 当前的时间步，通常是一个标量或形状为(batch_size,)的张量\n",
    "        \"\"\"\n",
    "\n",
    "        # 计算反向过程的均值μt\n",
    "        mean = xt - ((self.betas.to(xt.device)[t]) * noise_pred) / (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t])\n",
    "        mean = mean / torch.sqrt(self.alphas.to(xt.device)[t])\n",
    "        \n",
    "        # 根据时间步决定是否需要添加额外的噪声\n",
    "        if t == 0:\n",
    "            # 如果是最开始的时间步，则无需添加噪声，直接返回\n",
    "            return mean\n",
    "        else:\n",
    "            # 计算反向过程的方差作为噪声的权重，从而引入额外的噪声，使生成过程更加多样化\n",
    "            variance = (1 - self.alpha_cum_prod.to(xt.device)[t - 1]) / (1.0 - self.alpha_cum_prod.to(xt.device)[t])\n",
    "            variance = variance * self.betas.to(xt.device)[t]\n",
    "            sigma = variance ** 0.5\n",
    "            z = torch.randn(xt.shape).to(xt.device)\n",
    "            \n",
    "            return mean + sigma * z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义用来预测噪声的UNet网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 时间步正弦位置嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将时间步进行正弦位置嵌入.将离散的时间步转换为连续的、高维的向量表示\n",
    "def get_time_embedding(time_steps, temb_dim):\n",
    "    \"\"\"\n",
    "    time_steps: 一个形状为(batch_size,)的张量\n",
    "    temb_dim: 时间嵌入的维度，必须是偶数\n",
    "    \"\"\"\n",
    "\n",
    "    # 确保嵌入的维度是偶数\n",
    "    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n",
    "    \n",
    "    # factor = 10000^(2i/d_model)\n",
    "    factor = 10000 ** ((torch.arange(\n",
    "        start=0, end=temb_dim // 2, dtype=torch.float32, device=time_steps.device) / (temb_dim // 2))\n",
    "    )\n",
    "    \n",
    "    # pos / factor\n",
    "    # timesteps B -> B, 1 -> B, temb_dim\n",
    "    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n",
    "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n",
    "    return t_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下采样模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下采样模块\n",
    "class DownBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim,\n",
    "                 down_sample=True, num_heads=4, num_layers=1):\n",
    "        \"\"\"\n",
    "        in_channels: 输入特征图的通道数\n",
    "        out_channels: 输出特征图的通道数\n",
    "        t_emb_dim: 时间嵌入向量的维度\n",
    "        down_sample: 是否进行下采样。如果为True,则在最后通过卷积层将特征图尺寸缩小一半\n",
    "        num_heads: 注意力机制的头数\n",
    "        num_layers: 此块中的ResNet + 注意力层的数量\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.down_sample = down_sample\n",
    "        \n",
    "        # ResNet块的第一个卷积层\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 时间嵌入投影层，用于调整时间嵌入向量的维度\n",
    "        self.t_emb_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(t_emb_dim, out_channels)\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # ResNet块的第二个卷积层\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 自注意力机制\n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [nn.GroupNorm(8, out_channels)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "        self.attentions = nn.ModuleList(\n",
    "            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # 用于残差连接的1x1卷积\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 下采样\n",
    "        self.down_sample_conv = nn.Conv2d(out_channels, out_channels,\n",
    "                                          4, 2, 1) if self.down_sample else nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            # Resnet block of Unet\n",
    "            # 保存当前的out，作为残差连接的一部分\n",
    "            resnet_input = out\n",
    "\n",
    "            # 通过第一部分的ResNet块，进行归一化、激活和卷积操作\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "\n",
    "            # 将处理后的时间嵌入向量添加到out\n",
    "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "\n",
    "            # 通过第二部分的 ResNet 块，继续进行归一化、激活和卷积操作\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "\n",
    "            # 将之前保存的resnet_input通过1x1卷积层调整通道数后，与当前out进行残差连接\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "            \n",
    "\n",
    "            # Attention block of Unet\n",
    "            # 获取当前特征图的形状信息\n",
    "            batch_size, channels, h, w = out.shape\n",
    "\n",
    "            # 将特征图展平成 (batch_size, channels, h * w)，为注意力机制做准备\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "\n",
    "            # 对展平后的特征图进行组归一化\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "\n",
    "            # 转置为(batch_size, h * w, channels)，符合nn.MultiheadAttention的输入格式\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "\n",
    "            # 通过多头注意力机制处理特征图\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "\n",
    "            # 将注意力后的输出转置回(batch_size, channels, h * w)，然后重新reshape回(batch_size, channels, h, w)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "\n",
    "            # 将注意力后的输出与当前的out进行残差连接，增强特征表达\n",
    "            out = out + out_attn\n",
    "            \n",
    "        out = self.down_sample_conv(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 颈部模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads=4, num_layers=1):\n",
    "        \"\"\"\n",
    "        in_channels: 输入特征图的通道数\n",
    "        out_channels: 输出特征图的通道数\n",
    "        t_emb_dim: 时间嵌入向量的维度\n",
    "        num_heads: 注意力机制的头数\n",
    "        num_layers: 此块中的ResNet + 注意力层的数量\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # ResNet块的第一个卷积层\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers+1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 时间嵌入投影层\n",
    "        self.t_emb_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(t_emb_dim, out_channels)\n",
    "            )\n",
    "            for _ in range(num_layers + 1)\n",
    "        ])\n",
    "\n",
    "        # ResNet块的第二个卷积层\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers+1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # 自注意力模块\n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [nn.GroupNorm(8, out_channels)\n",
    "                for _ in range(num_layers)]\n",
    "        )\n",
    "        self.attentions = nn.ModuleList(\n",
    "            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # 用于残差连接的1x1卷积\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers+1)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "        \n",
    "        # First resnet block\n",
    "        # 保存当前的out，作为残差连接的一部分\n",
    "        resnet_input = out\n",
    "\n",
    "        # 通过第一个ResNet块的第一部分，进行组归一化、激活和卷积操作\n",
    "        out = self.resnet_conv_first[0](out)\n",
    "\n",
    "        # 将处理后的时间嵌入向量添加到out\n",
    "        out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n",
    "\n",
    "        # 通过第一个ResNet块的第二部分，继续进行组归一化、激活和卷积操作\n",
    "        out = self.resnet_conv_second[0](out)\n",
    "\n",
    "        # 将之前保存的resnet_input通过 1x1 卷积层调整通道数后，与当前out进行残差连接\n",
    "        out = out + self.residual_input_conv[0](resnet_input)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            # Attention Block\n",
    "            # 获取当前特征图的形状信息\n",
    "            batch_size, channels, h, w = out.shape\n",
    "\n",
    "            # 将特征图展平成(batch_size, channels, h * w)，为注意力机制做准备\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "\n",
    "            # 对展平后的特征图进行组归一化\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "\n",
    "            # 转置为(batch_size, h * w, channels)，符合nn.MultiheadAttention的输入格式\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "\n",
    "            # 通过多头注意力机制处理特征图\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "\n",
    "            # 将注意力后的输出转置回(batch_size, channels, h * w)，然后重新reshape回(batch_size, channels, h, w)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "\n",
    "            # 将注意力后的输出与当前的out进行残差连接，增强特征表达\n",
    "            out = out + out_attn\n",
    "            \n",
    "            # Resnet Block\n",
    "            # 保存当前的out，作为残差连接的一部分\n",
    "            resnet_input = out\n",
    "\n",
    "            # 通过ResNet块的第一部分，进行组归一化、激活和卷积操作\n",
    "            out = self.resnet_conv_first[i+1](out)\n",
    "\n",
    "            # 将处理后的时间嵌入向量添加到out\n",
    "            out = out + self.t_emb_layers[i+1](t_emb)[:, :, None, None]\n",
    "\n",
    "            # 通过ResNet块的第二部分，继续进行组归一化、激活和卷积操作\n",
    "            out = self.resnet_conv_second[i+1](out)\n",
    "\n",
    "            # 将之前保存的resnet_input通过1x1卷积层调整通道数后，与当前out进行残差连接\n",
    "            out = out + self.residual_input_conv[i+1](resnet_input)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上采样模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample=True, num_heads=4, num_layers=1):\n",
    "        \"\"\"\n",
    "        in_channels: 输入特征图的通道数\n",
    "        out_channels: 输出特征图的通道数\n",
    "        t_emb_dim: 时间嵌入向量的维度\n",
    "        up_sample: 是否进行上采样。如果为True,则在前向传播过程中通过反卷积将特征图的空间尺寸放大一倍\n",
    "        num_heads: 注意力机制的头数\n",
    "        num_layers: 此块中的ResNet + 注意力层的数量\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.up_sample = up_sample\n",
    "\n",
    "        # ResNet块的第一个卷积层\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 时间嵌入投影层\n",
    "        self.t_emb_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(t_emb_dim, out_channels)\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # ResNet块的第二个卷积层\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # 自注意力模块\n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [\n",
    "                nn.GroupNorm(8, out_channels)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.attentions = nn.ModuleList(\n",
    "            [\n",
    "                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 用于残差连接的1x1卷积\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 上采样层\n",
    "        self.up_sample_conv = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, 4, 2, 1) if self.up_sample else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, out_down, t_emb):\n",
    "        # 如果 up_sample=True，通过转置卷积将特征图的空间尺寸放大一倍\n",
    "        x = self.up_sample_conv(x)\n",
    "\n",
    "        # 上采样后的特征图x与下采样路径中的对应特征图out_down在通道维度上拼接\n",
    "        x = torch.cat([x, out_down], dim=1)\n",
    "        \n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "\n",
    "            # 保存当前的out，作为残差连接的一部分\n",
    "            resnet_input = out\n",
    "\n",
    "            # 通过ResNet块的第一部分，进行组归一化、激活和卷积操作\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "\n",
    "            # 将处理后的时间嵌入向量添加到out\n",
    "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "\n",
    "            # 通过ResNet块的第二部分，继续进行组归一化、激活和卷积操作\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "\n",
    "            # 将之前保存的resnet_input通过1x1卷积层调整通道数后，与当前out进行残差连接\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "            \n",
    "\n",
    "            # 获取当前特征图的形状信息\n",
    "            batch_size, channels, h, w = out.shape\n",
    "\n",
    "            # 将特征图展平成 (batch_size, channels, h * w)，为注意力机制做准备\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "\n",
    "            # 对展平后的特征图进行组归一化\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "\n",
    "            # 转置为(batch_size, h * w, channels)，符合nn.MultiheadAttention的输入格式\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "\n",
    "            # 通过多头注意力机制处理特征图\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "\n",
    "            # 将注意力后的输出转置回(batch_size, channels, h * w)，然后重新reshape回(batch_size, channels, h, w)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "\n",
    "            # 将注意力后的输出与当前的out进行残差连接，增强特征表达\n",
    "            out = out + out_attn\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet整体结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "\n",
    "    def __init__(self, model_config):\n",
    "        super().__init__()\n",
    "        im_channels = model_config['im_channels']\n",
    "        self.down_channels = model_config['down_channels']\n",
    "        self.mid_channels = model_config['mid_channels']\n",
    "        self.t_emb_dim = model_config['time_emb_dim']\n",
    "        self.down_sample = model_config['down_sample']\n",
    "        self.num_down_layers = model_config['num_down_layers']\n",
    "        self.num_mid_layers = model_config['num_mid_layers']\n",
    "        self.num_up_layers = model_config['num_up_layers']\n",
    "        \n",
    "        # 确保中间块（mid_channels）和下采样块（down_channels）的连接通道数匹配，并且下采样标记的数量正确\n",
    "        assert self.mid_channels[0] == self.down_channels[-1]\n",
    "        assert self.mid_channels[-1] == self.down_channels[-2]\n",
    "        assert len(self.down_sample) == len(self.down_channels) - 1\n",
    "        \n",
    "        # 时间嵌入投影\n",
    "        self.t_proj = nn.Sequential(\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n",
    "        )\n",
    "\n",
    "        self.up_sample = list(reversed(self.down_sample))\n",
    "\n",
    "        # 将原始图像的通道数映射到下采样路径所需的第一个通道数 \n",
    "        self.conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size=3, padding=(1, 1))\n",
    "        \n",
    "        # 下采样块列表\n",
    "        self.downs = nn.ModuleList([])\n",
    "        for i in range(len(self.down_channels)-1):\n",
    "            self.downs.append(DownBlock(self.down_channels[i], self.down_channels[i+1], self.t_emb_dim,\n",
    "                                        down_sample=self.down_sample[i], num_layers=self.num_down_layers))\n",
    "        # 中间块列表\n",
    "        self.mids = nn.ModuleList([])\n",
    "        for i in range(len(self.mid_channels)-1):\n",
    "            self.mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i+1], self.t_emb_dim,\n",
    "                                      num_layers=self.num_mid_layers))\n",
    "        # 上采样块列表\n",
    "        self.ups = nn.ModuleList([])\n",
    "        for i in reversed(range(len(self.down_channels)-1)):\n",
    "            self.ups.append(UpBlock(self.down_channels[i] * 2, self.down_channels[i-1] if i != 0 else 16,\n",
    "                                    self.t_emb_dim, up_sample=self.down_sample[i], num_layers=self.num_up_layers))\n",
    "        \n",
    "        # 输出层\n",
    "        self.norm_out = nn.GroupNorm(8, 16)\n",
    "        self.conv_out = nn.Conv2d(16, im_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        # 1) 输入卷积\n",
    "        out = self.conv_in(x)\n",
    "        \n",
    "        \n",
    "        # 2) 时间步嵌入\n",
    "        t_emb = get_time_embedding(torch.as_tensor(t).long(), self.t_emb_dim)\n",
    "        t_emb = self.t_proj(t_emb)\n",
    "        \n",
    "        # 3) 下采样路径\n",
    "        down_outs = []\n",
    "        for idx, down in enumerate(self.downs):\n",
    "            down_outs.append(out)\n",
    "            out = down(out, t_emb)\n",
    "\n",
    "        # 4) 中间块\n",
    "        for mid in self.mids:\n",
    "            out = mid(out, t_emb)\n",
    "\n",
    "\n",
    "        # 5) 上采样路径\n",
    "        for up in self.ups:\n",
    "            down_out = down_outs.pop()\n",
    "            out = up(out, down_out, t_emb)\n",
    "        \n",
    "        # 6) 最后输出\n",
    "        out = self.norm_out(out)\n",
    "        out = nn.SiLU()(out)\n",
    "        out = self.conv_out(out)\n",
    "        # out B x C x H x W\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义用于训练的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train(config_path):\n",
    "    with open(config_path, 'r') as file:\n",
    "        try:\n",
    "            config = yaml.safe_load(file)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "\n",
    "    diffusion_config = config['diffusion_params']\n",
    "    dataset_config = config['dataset_params']\n",
    "    model_config = config['model_params']\n",
    "    train_config = config['train_params']\n",
    "\n",
    "    datasets_name = dataset_config['datasets_name']\n",
    "\n",
    "    img_size = model_config['im_size']\n",
    "\n",
    "    # 是否需要训练\n",
    "    train_flag = train_config['train_flag']\n",
    "    if train_flag == False:\n",
    "        print(\"train_flag为False,不需要训练\")\n",
    "        return\n",
    "\n",
    "    # Create the noise scheduler\n",
    "    scheduler = LinearNoiseScheduler(num_timesteps=diffusion_config['num_timesteps'],\n",
    "                                     beta_start=diffusion_config['beta_start'],\n",
    "                                     beta_end=diffusion_config['beta_end'])\n",
    "    \n",
    "    # Create the dataset\n",
    "    if datasets_name == 'MNIST' or datasets_name == 'FashionMNIST':\n",
    "\n",
    "        transform = Compose([\n",
    "        ToTensor(),  # 转换为张量并归一化[0,1]\n",
    "        Lambda(lambda x: (x - 0.5) * 2)]  # 进一步将像素值从 [0, 1] 映射到 [-1, 1]\n",
    "        )\n",
    "\n",
    "        dataset = MNISTDataset(dataset_name=datasets_name, root=dataset_config['im_path'], transform=transform)\n",
    "\n",
    "        # 创建数据集加载器\n",
    "        loader = DataLoader(dataset, batch_size=train_config['batch_size'], shuffle=True)\n",
    "\n",
    "    elif datasets_name == 'Custom':\n",
    "        # 定义transform\n",
    "        transform = Compose([\n",
    "            Resize((img_size, img_size)),\n",
    "            ToTensor(),\n",
    "            Lambda(lambda x: (x - 0.5) * 2),\n",
    "        ])\n",
    "\n",
    "        dataset = MyCustomDataset(root=dataset_config['im_path'], transform=transform)\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=train_config['batch_size'], shuffle=True)   \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset_name. Choose either 'MNIST' or 'FashionMNIST' or 'Custom'.\")\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = Unet(model_config).to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # Create output directories\n",
    "    if not os.path.exists(train_config['task_name']):\n",
    "        os.mkdir(train_config['task_name'])\n",
    "\n",
    "    # Load checkpoint if found\n",
    "    model_name = f'ddpm_{datasets_name}.pth'\n",
    "    if os.path.exists(os.path.join(train_config['task_name'],model_name)):\n",
    "        print('Loading checkpoint as found one')\n",
    "        model.load_state_dict(torch.load(os.path.join(train_config['task_name'], model_name), map_location=device))\n",
    "    \n",
    "    # Specify training parameters\n",
    "    num_epochs = train_config['num_epochs']\n",
    "    optimizer = Adam(model.parameters(), lr=train_config['lr'])\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    # 初始化最优损失为正无穷大，用于保存损失最低的模型\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        losses = []\n",
    "        # 遍历每一个batch\n",
    "        for im in tqdm(loader, leave=False, desc=f\"Epoch {epoch_idx + 1}/{num_epochs}\", colour=\"#005500\"):\n",
    "            if datasets_name == 'MNIST' or datasets_name == 'FashionMNIST':\n",
    "                im = im[0].float().to(device)\n",
    "            elif datasets_name == 'Custom':\n",
    "                im = im.float().to(device)\n",
    "            else:\n",
    "                raise ValueError('图像维度错误')\n",
    "        \n",
    "\n",
    "            # Sample random noise\n",
    "            noise = torch.randn_like(im).to(device)\n",
    "\n",
    "            # Sample timestep\n",
    "            t = torch.randint(0, diffusion_config['num_timesteps'], (im.shape[0],)).to(device)\n",
    "\n",
    "            # Add noise to images according to timestep\n",
    "            noisy_im = scheduler.add_noise(im, noise, t)\n",
    "            noise_pred = model(noisy_im, t)\n",
    "\n",
    "            loss = criterion(noise_pred, noise)\n",
    "            losses.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # 计算每个epoch的平均损失\n",
    "        epoch_loss = np.mean(losses)\n",
    "\n",
    "        log_string = f\"Loss at epoch {epoch_idx + 1}: {epoch_loss:.3f}\"\n",
    "\n",
    "        # 保存最好的模型\n",
    "        if best_loss > epoch_loss:\n",
    "            best_loss = epoch_loss\n",
    "            model_name = f'ddpm_{datasets_name}.pth'\n",
    "            torch.save(model.state_dict(), os.path.join(train_config['task_name'], model_name))\n",
    "            \n",
    "            log_string += \" --> Best model ever (stored)\"\n",
    "\n",
    "        print(log_string)\n",
    "    \n",
    "    print('Done Training ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义用于推理的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def sample(model, scheduler, train_config, model_config, diffusion_config):\n",
    "    \"\"\"\n",
    "    model:神经网络模型\n",
    "    scheduler:扩散过程的调度器\n",
    "    train_config:训练或采样的配置字典\n",
    "    model_config:模型的配置字典\n",
    "    diffusion_config:扩散过程的配置字典\n",
    "    \"\"\"\n",
    "    # 动画GIF的帧数\n",
    "    frames_per_gif=100\n",
    "\n",
    "    # 从0到n_steps生成frames_per_gif个帧索引\n",
    "    frame_idxs = np.linspace(0, diffusion_config['num_timesteps'] - 1, frames_per_gif, dtype=int)\n",
    "    frame_idxs = sorted(set(frame_idxs))  # 确保唯一且有序\n",
    "\n",
    "    # 存储生成过程中的图像帧\n",
    "    frames = []\n",
    "\n",
    "    c, h, w = model_config['im_channels'], model_config['im_size'], model_config['im_size']\n",
    "\n",
    "    # 随机初始化xt\n",
    "    xt = torch.randn((train_config['num_samples'],\n",
    "                      model_config['im_channels'],\n",
    "                      model_config['im_size'],\n",
    "                      model_config['im_size'])).to(device)\n",
    "\n",
    "    # 逆向扩散循环\n",
    "    for i in tqdm(reversed(range(diffusion_config['num_timesteps'])), desc=\"generating images progess\", leave=False, colour=\"#005500\"):\n",
    "        # 预测噪声\n",
    "        noise_pred = model(xt, torch.as_tensor(i).unsqueeze(0).to(device))\n",
    "        \n",
    "        # 通过调度器（scheduler）从 xtxt​ 计算 xt−1xt−1​\n",
    "        xt = scheduler.sample_prev_timestep(xt, noise_pred, torch.as_tensor(i).to(device))\n",
    "\n",
    "        # 保存生成过程的关键帧\n",
    "        if i in frame_idxs or i == 0: \n",
    "\n",
    "            # 保存最后一步去噪的结果\n",
    "            if i == 0:\n",
    "                x0 = xt.clone()\n",
    "\n",
    "            # 将图片像素值从[-1, 1]转换到[0, 255]范围\n",
    "            normalized = (xt.clone() + 1) / 2  # [0,1]\n",
    "            normalized = normalized.clamp(0, 1)\n",
    "            normalized = (normalized * 255).byte()  # [0,255]并转为uint8\n",
    "\n",
    "            # 转换为NumPy数组\n",
    "            frame = normalized.cpu().numpy()\n",
    "\n",
    "            if c == 1:\n",
    "                # 如果是单通道，移除最后一个维度\n",
    "                frame = np.squeeze(frame, axis=1)  # (B, H, W)\n",
    "            else:\n",
    "                # 多通道，调整为 (B, H, W, C)\n",
    "                frame = np.transpose(frame, (0, 2, 3, 1))  # (B, H, W, C)\n",
    "\n",
    "            # 创建图像网格，例如4x4\n",
    "            grid_size = int(np.sqrt(train_config['num_samples']))\n",
    "            if grid_size ** 2 != train_config['num_samples']:\n",
    "                grid_size = int(np.ceil(np.sqrt(train_config['num_samples'])))\n",
    "\n",
    "            \n",
    "            # 填充不足的图像\n",
    "            if train_config['num_samples'] < grid_size ** 2:\n",
    "                padding = grid_size ** 2 - train_config['num_samples']\n",
    "                if c == 1:\n",
    "                    pad_img = np.zeros((padding, h, w), dtype=np.uint8)\n",
    "                else:\n",
    "                    pad_img = np.zeros((padding, h, w, c), dtype=np.uint8)\n",
    "                frame = np.concatenate([frame, pad_img], axis=0)\n",
    "            \n",
    "            # 使用einops将图像排列成网格\n",
    "            if c == 1:\n",
    "                # 对于单通道，保持为二维数组\n",
    "                frame = rearrange(frame, '(b1 b2) h w -> (b1 h) (b2 w)', b1=grid_size)\n",
    "                # 现在frame是(H_total, W_total)，单通道\n",
    "            else:\n",
    "                frame = rearrange(frame, '(b1 b2) h w c -> (b1 h) (b2 w) c', b1=grid_size)\n",
    "\n",
    "            # 添加到帧列表\n",
    "            frames.append(frame)\n",
    "    \n",
    "    # 保存为GIF\n",
    "    gif_path = os.path.join(train_config['task_name'], 'samples', \"ddpm_mydata.gif\")\n",
    "    os.makedirs(os.path.dirname(gif_path), exist_ok=True)\n",
    "    \n",
    "    # 保存为GIF\n",
    "    with imageio.get_writer(gif_path, mode=\"I\") as writer:\n",
    "        for frame in frames:\n",
    "            if c == 1:\n",
    "                # imageio expects (H, W) for grayscale\n",
    "                writer.append_data(frame)\n",
    "            else:\n",
    "                # For RGB or other multi-channel images\n",
    "                writer.append_data(frame)\n",
    "        # 在最后一帧停留更长时间\n",
    "        for _ in range(frames_per_gif // 3):\n",
    "            writer.append_data(frames[-1])\n",
    "    \n",
    "    print(f'结果保存在:{gif_path}')\n",
    "\n",
    "    # 返回最后一步去噪的结果\n",
    "    return x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(config_path):\n",
    "    # 读取配置文件\n",
    "    with open(config_path, 'r') as file:\n",
    "        try:\n",
    "            config = yaml.safe_load(file)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    \n",
    "    diffusion_config = config['diffusion_params']\n",
    "    model_config = config['model_params']\n",
    "    train_config = config['train_params']\n",
    "    dataset_config = config['dataset_params']\n",
    "\n",
    "    datasets_name = dataset_config['datasets_name']\n",
    "    \n",
    "    # 加载模型\n",
    "    model_name = f'ddpm_{datasets_name}.pth'\n",
    "    model = Unet(model_config).to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(train_config['task_name'], model_name), map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    # 创建噪声调度器\n",
    "    scheduler = LinearNoiseScheduler(num_timesteps=diffusion_config['num_timesteps'],\n",
    "                                     beta_start=diffusion_config['beta_start'],\n",
    "                                     beta_end=diffusion_config['beta_end'])\n",
    "\n",
    "    # 推理\n",
    "    with torch.no_grad():\n",
    "        generated = sample(model, scheduler, train_config, model_config, diffusion_config)\n",
    "    \n",
    "    # 返回最后一步去噪的结果\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义用于可视化图片的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, title=\"\"):\n",
    "    \"\"\"\n",
    "    images: 一个包含多张RGB或灰度图片的输入，形状 (B, C, H, W)\n",
    "    title: 用来给显示的图片设置标题\n",
    "    \"\"\"\n",
    "    # 将图片转换为numpy数组\n",
    "    if isinstance(images, torch.Tensor):\n",
    "        images = images.detach().cpu().numpy()\n",
    "    \n",
    "    # 检查通道数并转换为(B, H, W, C)或(B, H, W)\n",
    "    if images.shape[1] == 3:\n",
    "        images = images.transpose(0, 2, 3, 1)  # (B, H, W, 3)\n",
    "        is_grayscale = False\n",
    "    elif images.shape[1] == 1:\n",
    "        images = images.squeeze(1)  # (B, H, W)\n",
    "        is_grayscale = True\n",
    "    else:\n",
    "        raise ValueError(\"图像维度错误\")\n",
    "    \n",
    "    # 将像素值从 [-1, 1] 映射到 [0, 255]\n",
    "    images = (images + 1) / 2 * 255\n",
    "    images = np.clip(images, 0, 255).astype(np.uint8)  # 确保值在 [0, 255] 并转换为uint8\n",
    "    \n",
    "    # 定义显示网格的行数和列数\n",
    "    num_images = len(images)\n",
    "    rows = int(np.sqrt(num_images))\n",
    "    cols = int(np.ceil(num_images / rows))\n",
    "\n",
    "    # 创建带有黑色背景的图形\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2), facecolor='black')\n",
    "    fig.suptitle(title, fontsize=30, color='white')\n",
    "\n",
    "    # 如果只有一个子图，axes不再是二维数组\n",
    "    if rows * cols == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif rows == 1 or cols == 1:\n",
    "        axes = axes.reshape(rows, cols)\n",
    "    \n",
    "    # 填充网格并显示图片\n",
    "    idx = 0\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            ax = axes[r, c]\n",
    "            ax.set_facecolor('black')  # 设置子图背景为黑色\n",
    "            ax.axis(\"off\")  # 去掉坐标轴\n",
    "            if idx < num_images:\n",
    "                if is_grayscale:\n",
    "                    ax.imshow(images[idx], cmap='gray', vmin=0, vmax=255)\n",
    "                else:\n",
    "                    ax.imshow(images[idx])\n",
    "                idx += 1\n",
    "            else:\n",
    "                ax.remove()  # 如果没有更多的图片，移除多余的子图\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.90)  # 调整顶部以适应标题\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化数据集的第一个batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化第一个batch的函数\n",
    "def show_first_batch(config_path):\n",
    "    with open(config_path, 'r') as file:\n",
    "        try:\n",
    "            config = yaml.safe_load(file)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    \n",
    "    diffusion_config = config['diffusion_params']\n",
    "    dataset_config = config['dataset_params']\n",
    "    model_config = config['model_params']\n",
    "    train_config = config['train_params']\n",
    "\n",
    "    datasets_name = dataset_config['datasets_name']\n",
    "\n",
    "    img_size = model_config['im_size']\n",
    "\n",
    "    # Create the dataset\n",
    "    if datasets_name == 'MNIST' or datasets_name == 'FashionMNIST':\n",
    "\n",
    "        transform = Compose([\n",
    "        ToTensor(),  # 转换为张量并归一化[0,1]\n",
    "        Lambda(lambda x: (x - 0.5) * 2)]  # 进一步将像素值从 [0, 1] 映射到 [-1, 1]\n",
    "        )\n",
    "\n",
    "        dataset = MNISTDataset(dataset_name=datasets_name, root=dataset_config['im_path'], transform=transform)\n",
    "\n",
    "        # 创建数据集加载器\n",
    "        loader = DataLoader(dataset, batch_size=train_config['batch_size'], shuffle=True)\n",
    "\n",
    "    elif datasets_name == 'Custom':\n",
    "        # 定义transform\n",
    "        transform = Compose([\n",
    "            Resize((img_size, img_size)),\n",
    "            ToTensor(),\n",
    "            Lambda(lambda x: (x - 0.5) * 2),\n",
    "        ])\n",
    "\n",
    "        dataset = MyCustomDataset(root=dataset_config['im_path'], transform=transform)\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=train_config['batch_size'], shuffle=True)   \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset_name. Choose either 'MNIST' or 'FashionMNIST' or 'Custom'.\")\n",
    "    \n",
    "    \n",
    "    for batch in loader:\n",
    "        if datasets_name == 'MNIST' or datasets_name == 'FashionMNIST':\n",
    "            print(batch[0].shape)\n",
    "            show_images(batch[0], \"Images in the first batch\")\n",
    "        elif datasets_name == 'Custom':\n",
    "            print(batch.shape)\n",
    "            show_images(batch, \"Images in the first batch\")\n",
    "        else:\n",
    "            raise ValueError(\"图像维度错误\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'cfg.yaml'\n",
    "show_first_batch(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化前向加噪过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_forward(config_path):\n",
    "    num_timesteps = 1000\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    with open(config_path, 'r') as file:\n",
    "        try:\n",
    "            config = yaml.safe_load(file)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    \n",
    "    diffusion_config = config['diffusion_params']\n",
    "    dataset_config = config['dataset_params']\n",
    "    model_config = config['model_params']\n",
    "    train_config = config['train_params']\n",
    "\n",
    "    datasets_name = dataset_config['datasets_name']\n",
    "\n",
    "    img_size = model_config['im_size']\n",
    "\n",
    "    # Create the dataset\n",
    "    if datasets_name == 'MNIST' or datasets_name == 'FashionMNIST':\n",
    "\n",
    "        transform = Compose([\n",
    "        ToTensor(),  # 转换为张量并归一化[0,1]\n",
    "        Lambda(lambda x: (x - 0.5) * 2)]  # 进一步将像素值从 [0, 1] 映射到 [-1, 1]\n",
    "        )\n",
    "\n",
    "        dataset = MNISTDataset(dataset_name=datasets_name, root=dataset_config['im_path'], transform=transform)\n",
    "\n",
    "        # 创建数据集加载器\n",
    "        loader = DataLoader(dataset, batch_size=train_config['batch_size'], shuffle=True)\n",
    "\n",
    "    elif datasets_name == 'Custom':\n",
    "        # 定义transform\n",
    "        transform = Compose([\n",
    "            Resize((img_size, img_size)),\n",
    "            ToTensor(),\n",
    "            Lambda(lambda x: (x - 0.5) * 2),\n",
    "        ])\n",
    "\n",
    "        dataset = MyCustomDataset(root=dataset_config['im_path'], transform=transform)\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=train_config['batch_size'], shuffle=True)   \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset_name. Choose either 'MNIST' or 'FashionMNIST' or 'Custom'.\")\n",
    "    \n",
    "\n",
    "    for batch in loader:\n",
    "        if datasets_name == 'MNIST' or datasets_name == 'FashionMNIST':\n",
    "            imgs = batch[0].to(device)\n",
    "        elif datasets_name == 'Custom':\n",
    "            imgs = batch.to(device)\n",
    "        else:\n",
    "            raise ValueError(\"图像维度错误\")\n",
    "            \n",
    "\n",
    "        n, c, h, w = imgs.shape\n",
    "\n",
    "        # 可视化原始图像\n",
    "        show_images(imgs, \"Original Images\")\n",
    "\n",
    "        # 定义需要进行可视化的噪声比例\n",
    "        percents = [0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "        scheduler = LinearNoiseScheduler(num_timesteps, beta_start, beta_end)\n",
    "\n",
    "        for percent in percents:\n",
    "            # 计算对应的时间步，确保不超出范围\n",
    "            t_step = max(0, min(int(percent * num_timesteps) - 1, num_timesteps - 1))\n",
    "\n",
    "            # 创建一个与批量大小相同的时间步张量\n",
    "            t = torch.full((n,), t_step, device=device, dtype=torch.long)\n",
    "\n",
    "            eta = torch.randn(n, c, h, w).to(device)\n",
    "            \n",
    "            # 进行前向加噪过程\n",
    "            noisy_image = scheduler.add_noise(original=imgs, noise=eta, t=t)\n",
    "\n",
    "            # 可视化带噪声的图像\n",
    "            show_images(noisy_image, f\"DDPM Noisy Images {int(percent * 100)}%\")\n",
    "\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'cfg.yaml'\n",
    "show_forward(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"cfg.yaml\"\n",
    "\n",
    "train(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用训练好的模型进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"cfg.yaml\"\n",
    "generated = infer(config_path)\n",
    "show_images(generated, \"Final result\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DDPM2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
